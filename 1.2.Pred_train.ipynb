{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"1.2.Pred_train.ipynb","provenance":[],"authorship_tag":"ABX9TyNaD8LzwY7z5vtBGc3ltR2Z"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"2KR7cfmwbuZP","colab_type":"code","colab":{}},"source":["from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","import tensorflow as tf\n","import cv2\n","import os\n","import numpy as np\n","from easydict import EasyDict as edict\n","import tensorflow.contrib.slim as slim\n","\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","\n","__C = edict()\n","cfg = __C\n","__C.DIR = '/content/drive/My Drive/VSimplE/'\n","__C.POOLING_SIZE = 7\n","__C.VRD_BATCH_NUM = 30\n","__C.VRD_NUM_CLASS = 101\n","__C.VRD_NUM_RELA = 70\n","__C.VRD_LR_INIT = 0.00001\n","__C.VRD_TRAIN_ROUND = 20\n","__C.VRD_BATCH_NUM_RELA = 50\n","__C.VRD_AU_PAIR = 5\n","__C.VRD_IOU_TRAIN = 0.5\n","__C.VRD_IOU_TEST = 0.5\n","__C.IM_SIZE = 600\n","__C.IM_MAX_SIZE = 1000\n","__C.TRAIN = edict()\n","__C.RESNET = edict()\n","__C.RESNET.FIXED_BLOCKS = 1\n","__C.VTR = edict()\n","\n","print (\"tf  :       \",tf.__version__)\n","print (\"cv2 :       \",cv2.__version__)\n","print (\"np  :       \",np.__version__)\n","\n","\n","def generate_batch(N_total, N_each):\n","    num_batch = np.int32(N_total / N_each)\n","    if N_total % N_each == 0:\n","        index_box = range(N_total)\n","    else:\n","        index_box = np.empty(shape=[N_each * (num_batch + 1)], dtype=np.int32)\n","        index_box[0:N_total] = range(N_total)\n","        N_rest = N_each * (num_batch + 1) - N_total\n","        index_box[N_total:] = np.random.randint(0, N_total, N_rest)\n","    return index_box\n","\n","def read_roidb(roidb_path):\n","    roidb_file = np.load(roidb_path, allow_pickle=True)\n","    key = list(roidb_file.keys())[0]\n","    roidb_temp = roidb_file[key]\n","    roidb = roidb_temp[()]\n","    return roidb\n","#_______________________________________________________________________________\n","co_occurency_tensor = read_roidb(cfg.DIR + 'input/co_occurency_tensor.npz')\n","P_op = read_roidb(cfg.DIR + 'input/P_op.npz')\n","P_sp = read_roidb(cfg.DIR + 'input/P_sp.npz')\n","P_p = read_roidb(cfg.DIR  + 'input/P_p.npz')\n","\n","def im_preprocess(image_path):\n","    image = cv2.imread(image_path)\n","    im_orig = image.astype(np.float32, copy=True)\n","    im_orig -= np.array([[[102.9801, 115.9465, 122.7717]]])\n","    im_shape = im_orig.shape\n","    im_size_min = np.min(im_shape[0:2])\n","    im_size_max = np.max(im_shape[0:2])\n","    target_size = cfg.IM_SIZE\n","    max_size = cfg.IM_MAX_SIZE\n","    im_scale = float(target_size) / float(im_size_min)\n","    if np.round(im_scale * im_size_max) > max_size:\n","        im_scale = float(max_size) / float(im_size_max)\n","    im = cv2.resize(im_orig, None, None, fx=im_scale, fy=im_scale, interpolation=cv2.INTER_LINEAR)\n","    im_shape_new = np.shape(im)\n","    im_use = np.zeros([1, im_shape_new[0], im_shape_new[1], im_shape_new[2]])\n","    im_use[0, :, :, :] = im\n","    return im_use, im_scale    \n","\n","########################## spatial_vector     ###################################\n","def spatial_vector(sbbox, obbox, N_each_batch):\n","    spatial_vector = np.zeros([N_each_batch, 26])\n","    ubbox = generate_phrase_box(sbbox, obbox)\n","    for i in range(N_each_batch):\n","        x_s = (sbbox[i][0] + sbbox[i][2]) / 2.0\n","        y_s = (sbbox[i][1] + sbbox[i][3]) / 2.0\n","        w_s = sbbox[i][2] - sbbox[i][0]\n","        h_s = sbbox[i][3] - sbbox[i][1]\n","\n","        x_o = (obbox[i][0] + obbox[i][2]) / 2.0\n","        y_o = (obbox[i][1] + obbox[i][3]) / 2.0\n","        w_o = obbox[i][2] - obbox[i][0]\n","        h_o = obbox[i][3] - obbox[i][1]\n","\n","        x_u = (ubbox[i][0] + ubbox[i][2]) / 2.0\n","        y_u = (ubbox[i][1] + ubbox[i][3]) / 2.0\n","        w_u = ubbox[i][2] - ubbox[i][0]\n","        h_u = ubbox[i][3] - ubbox[i][1]\n","        spatial_vector[i] = [x_s, y_s, w_s / w_o, h_s / h_o, ((w_s * h_s) / (w_o * h_o)),\n","                            (x_s - x_o) / w_o, (y_s - y_o) / h_o, w_s / 600, h_s / 600,\n","                            np.log(w_s / w_o), np.log(h_s / h_o),\n","\n","                            x_u, y_u, w_s / w_u, h_s / h_u, (w_s * h_s) / (w_u * h_u),\n","                            (x_s - x_u) / w_u, (y_s - y_u) / h_u, w_u / 600, h_u / 600,\n","                            np.log(w_s / w_u), np.log(h_s / h_u),\n","\n","                            (sbbox[i][0] - ubbox[i][0]) / w_u, (sbbox[i][1] - ubbox[i][1]) / h_u,\n","                            (sbbox[i][2] - ubbox[i][2]) / w_u, (sbbox[i][3] - ubbox[i][3]) / h_u\n","                             ]\n","    return spatial_vector\n","#####################################################     IoU      ###################################################\n","def compute_iou_batch(sbbox, obbox, N_each_batch):\n","    iou = np.zeros([N_each_batch, 4])\n","    for i in range(N_each_batch):\n","        xA = max(sbbox[i][0], obbox[i][0])\n","        yA = max(sbbox[i][1], obbox[i][1])\n","        xB = min(sbbox[i][2], obbox[i][2])\n","        yB = min(sbbox[i][3], obbox[i][3])\n","        interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n","        sbbox_Area = (sbbox[i][2] - sbbox[i][0] + 1) * (sbbox[i][3] - sbbox[i][1] + 1)\n","        obbox_Area = (obbox[i][2] - obbox[i][0] + 1) * (obbox[i][3] - obbox[i][1] + 1)\n","        iou[i] = interArea / float(sbbox_Area + obbox_Area - interArea)\n","    return iou\n","\n","#####################\tco_occurency\t##############################\n","def co_occurency(sub_gt,ob_gt,N_each_batch):\n","\tco_occurency_list = np.zeros((30,70))\n","\tfor i in range(N_each_batch):\n","\t\tsub_idx = sub_gt[i]\n","\t\tob_idx = ob_gt[i]\n","\t\tco_occurency_temp = np.zeros((70))\n","\t\tfor j in range (70):\n","\t\t\tPMI = np.log10(((co_occurency_tensor[sub_idx][ob_idx][j]) * P_p[j]) / (P_sp[sub_idx][j] * P_op[ob_idx][j]))\n","\t\t\tif PMI > 0 :\n","\t\t\t\tco_occurency_temp[j] = PMI\n","\t\t\telse:\n","\t\t\t\tco_occurency_temp[j] = 0\t\n","\t\tco_occurency_list[i] = co_occurency_temp\n","\treturn co_occurency_list\n","##################################################\n","def get_blob_pred(roidb_use, im_scale, index_sp, N_each_batch, batch_id):\n","    blob = {}\n","    sub_box = roidb_use['sub_box_gt']*im_scale\n","    obj_box = roidb_use['obj_box_gt']*im_scale\n","    rela = np.int32(roidb_use['rela_gt'])\n","    index = roidb_use['index_pred']\n","    sub_gt = np.int32(roidb_use['sub_gt'])\n","    ob_gt = np.int32(roidb_use['obj_gt'])\n","    index_use = index[batch_id*N_each_batch: (batch_id+1)*N_each_batch]\n","    sub_gt_use = sub_gt[index_use]\n","    ob_gt_use = ob_gt[index_use]\n","    sub_box_use = sub_box[index_use,:]\n","    obj_box_use = obj_box[index_use,:]\n","    rela_use = rela[index_use]\n","    co_occurency_log = co_occurency(sub_gt_use, ob_gt_use, N_each_batch)\n","    blob['sub_label'] = sub_gt_use\n","    blob['obj_label'] = ob_gt_use\n","    blob['sub_box'] = sub_box_use\n","    blob['obj_box'] = obj_box_use\n","    blob['rela'] = rela_use\n","    blob['iou'] = compute_iou_batch(sub_box_use , obj_box_use , N_each_batch)\n","    blob['union_box'] = generate_phrase_box(sub_box_use , obj_box_use)\n","    blob['sub_spatial_vector'] = spatial_vector(sub_box_use, obj_box_use ,N_each_batch)\n","    blob['ob_spatial_vector'] = spatial_vector(obj_box_use, sub_box_use ,N_each_batch)\n","    blob['co_occurency_tensor'] = co_occurency_log\n","    return blob\n","############################################################################################################\n","def generate_phrase_box(sbox, obox):\n","    N_box = len(sbox)\n","    phrase = np.zeros([N_box, 4])\n","    for i in range(N_box):\n","        phrase[i, 0] = min(sbox[i, 0], obox[i, 0])\n","        phrase[i, 1] = min(sbox[i, 1], obox[i, 1])\n","        phrase[i, 2] = max(sbox[i, 2], obox[i, 2])\n","        phrase[i, 3] = max(sbox[i, 2], obox[i, 3])\n","    return phrase\n","# ________________________________________________________________________________________________________#\n","class VSimplE(object):\n","    def __init__(self):\n","        self.predictions = {}\n","        self.losses = {}\n","        self.layers = {}\n","        self.feat_stride = [16, ]\n","        self.scope = 'vgg_16'\n","\n","    def create_graph(self, N_each_batch, index_sp, index_cls, num_classes, num_predicates):\n","        self.image      = tf.placeholder(tf.float32, shape=[1, None, None, 3])\n","        self.sub_label  = tf.placeholder(tf.int32, shape=[N_each_batch, ])\n","        self.obj_label  = tf.placeholder(tf.int32, shape=[N_each_batch, ])        \n","        self.sbox       = tf.placeholder(tf.float32, shape=[N_each_batch, 4])\n","        self.obox       = tf.placeholder(tf.float32, shape=[N_each_batch, 4])\n","        self.iou        = tf.placeholder(tf.float32 , shape=[N_each_batch , 4] )\n","        self.union_box  = tf.placeholder(tf.float32, shape=[N_each_batch, 4])\n","        self.sub_sp     = tf.placeholder(tf.float32, shape=[N_each_batch, 26])\n","        self.ob_sp      = tf.placeholder(tf.float32, shape=[N_each_batch, 26])\n","        self.co_occurency_log = tf.placeholder(tf.float32 , shape=[N_each_batch , 70] )\n","        self.rela_label = tf.placeholder(tf.int32, shape=[N_each_batch, ])\n","        self.keep_prob  = tf.placeholder(tf.float32)\n","\n","        self.sub_obj_p = tf.placeholder(tf.float32, shape=[None, num_predicates])\n","        self.object_word = np.load(cfg.DIR +'/input/oList_word_embedding.npy')\n","        self.robject = np.load(cfg.DIR +'/input/rList_word_embedding.npy')\n","        conf = np.load(cfg.DIR +'/input/language_inter.npz')\n","        self.sub_obj_pred = conf['sub_obj']\n","        self.sub_pred = conf['sub']\n","        self.obj_pred = conf['obj']\n","\n","        self.index_sp = index_sp\n","        self.index_cls = index_cls\n","        self.num_classes = num_classes\n","        self.num_predicates = num_predicates\n","        self.N_each_batch = N_each_batch\n","\n","        self.build_dete_network()\n","        self.setup_ent_emb()\n","        self.build_rd_network()\n","        self.add_rd_loss()\n","\n","#################################__________VGG 16____________################################################\n","\n","    def build_dete_network(self, is_training=True):\n","        img= self.image\n","        net_conv = self.image_to_head(is_training)\n","        sub_pool5 = self.crop_pool_layer(net_conv, self.sbox, \"sub_pool5\")\n","        ob_pool5 = self.crop_pool_layer(net_conv, self.obox, \"ob_pool5\")\n","        sub_fc7 = self.head_to_tail(sub_pool5, is_training, reuse=False)\n","        ob_fc7 = self.head_to_tail(ob_pool5, is_training, reuse=True)\n","        with tf.variable_scope(self.scope, self.scope):\n","            sub_cls_prob, sub_cls_pred = self.region_classification(sub_fc7, is_training, reuse=False)\n","        with tf.variable_scope(self.scope, self.scope):\n","            ob_cls_prob, ob_cls_pred = self.region_classification(ob_fc7, is_training, reuse=True)\n","\n","        self.predictions['sub_cls_prob'] = sub_cls_prob\n","        self.predictions['sub_cls_pred'] = sub_cls_pred\n","        self.predictions['ob_cls_prob'] = ob_cls_prob\n","        self.predictions['ob_cls_pred'] = ob_cls_pred\n","        # self.layers['sub_pool5'] = sub_pool5\n","        # self.layers['ob_pool5'] = ob_pool5\n","        self.layers['sub_fc7'] = sub_fc7\n","        self.layers['ob_fc7'] = ob_fc7\n","\n","    def image_to_head(self, is_training, reuse=False):\n","        with tf.variable_scope(self.scope, self.scope, reuse=reuse):\n","            print(\"Image_shape :  \",self.image)\n","            net = slim.repeat(self.image, 2, slim.conv2d, 64, [3, 3],trainable=is_training, scope='conv1')\n","            net = slim.max_pool2d(net, [2, 2], padding='SAME', scope='pool1')\n","            net = slim.repeat(net, 2, slim.conv2d, 128, [3, 3],trainable=is_training, scope='conv2')\n","            net = slim.max_pool2d(net, [2, 2], padding='SAME', scope='pool2')\n","            net = slim.repeat(net, 3, slim.conv2d, 256, [3, 3],trainable=is_training, scope='conv3')\n","            net = slim.max_pool2d(net, [2, 2], padding='SAME', scope='pool3')\n","            net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3],trainable=is_training, scope='conv4')\n","            net = slim.max_pool2d(net, [2, 2], padding='SAME', scope='pool4')\n","            net_conv = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], trainable=is_training, scope='conv5')\n","            self.layers['head'] = net_conv\n","            return net_conv\n","\n","    def head_to_tail(self, pool5, is_training, reuse=False):\n","        with tf.variable_scope(self.scope, self.scope, reuse=reuse):\n","            pool5_flat = slim.flatten(pool5, scope='flatten')\n","            fc6 = slim.fully_connected(pool5_flat, 4096, scope='fc6')\n","            fc6 = slim.dropout(fc6, keep_prob=self.keep_prob, is_training=True,scope='dropout6')\n","            fc7 = slim.fully_connected(fc6, 4096, scope='fc7')\n","            fc7 = slim.dropout(fc7, keep_prob=self.keep_prob, is_training=True,scope='dropout7')\n","            return fc7\n","\n","    def crop_pool_layer(self, bottom, rois, name):\n","        with tf.variable_scope(name) as scope:\n","            n = tf.to_int32(rois.shape[0])\n","            batch_ids = tf.zeros([n, ], dtype=tf.int32)\n","            bottom_shape = tf.shape(bottom)\n","            height = (tf.to_float(bottom_shape[1]) - 1.) * np.float32(self.feat_stride[0])\n","            width = (tf.to_float(bottom_shape[2]) - 1.) * np.float32(self.feat_stride[0])\n","            x1 = tf.slice(rois, [0, 0], [-1, 1], name=\"x1\") / width\n","            y1 = tf.slice(rois, [0, 1], [-1, 1], name=\"y1\") / height\n","            x2 = tf.slice(rois, [0, 2], [-1, 1], name=\"x2\") / width\n","            y2 = tf.slice(rois, [0, 3], [-1, 1], name=\"y2\") / height\n","            bboxes = tf.stop_gradient(tf.concat([y1, x1, y2, x2], 1))\n","            crops = tf.image.crop_and_resize(bottom, bboxes, tf.to_int32(batch_ids),[cfg.POOLING_SIZE * 2, cfg.POOLING_SIZE * 2], method='bilinear',name=\"crops\")\n","            pooling = max_pool(crops, 2, 2, 2, 2, name=\"max_pooling\")\n","        return pooling\n","\n","    def region_classification(self, fc7, is_training, reuse=False):\n","        cls_score = slim.fully_connected(fc7, self.num_classes,activation_fn=None, scope='cls_score', reuse=reuse)\n","        print(\"cls_score's shape: {0}\".format(cls_score.get_shape()))\n","        cls_prob = tf.nn.softmax(cls_score, name=\"cls_prob\")\n","        cls_pred = tf.argmax(cls_score, axis=1, name=\"cls_pred\")\n","        return cls_prob, cls_pred\n","        \n","############################################      Relationship detection          ###########################################################\n","    def setup_ent_emb(self):\n","        self.head = tf.get_variable(name=\"RD_head_emb\",initializer=tf.random_uniform(shape=[30, 2000], minval=0, maxval=1999,dtype=tf.int32))\n","        self.tail = tf.get_variable(name=\"RD_tail_emb\",initializer=tf.random_uniform(shape=[30, 2000], minval=0, maxval=1999,dtype=tf.int32))\n","\n","    def Bi_GRU(self, inputs, name):\n","        with tf.variable_scope(name):\n","            fw_gru_cell = tf.contrib.rnn.GRUCell(1000)\n","            bw_gru_cell = tf.contrib.rnn.GRUCell(1000)\n","            fw_gru_cell = tf.contrib.rnn.DropoutWrapper(fw_gru_cell, output_keep_prob=0.8)\n","            bw_gru_cell = tf.contrib.rnn.DropoutWrapper(bw_gru_cell, output_keep_prob=0.8)\n","            (fw_outputs, bw_outputs),_ = tf.nn.bidirectional_dynamic_rnn(cell_fw=fw_gru_cell,cell_bw=bw_gru_cell,inputs=inputs,dtype=tf.float32)\n","            outputs = tf.concat((fw_outputs, bw_outputs), 2)\n","            outputs = tf.reshape(outputs,[30,2000])\n","        return outputs\n","\n","    def max_norm_regularizer(self, threshold, axes=1, name=\"max_norm\", collection=\"max_norm\"):\n","        def max_norm(weights):\n","            clipped = tf.clip_by_norm(weights, clip_norm=threshold, axes=axes)\n","            clip_weights = tf.assign(weights, clipped, name=name)\n","            tf.add_to_collection(collection, clip_weights)\n","            return None\n","        return max_norm\n","\n","    def build_rd_network(self):\n","        #_________________________Sub_feature\n","        sub_cls_prob = self.predictions['sub_cls_prob']\n","        sub_cls_pred = self.predictions['sub_cls_pred']\n","        sub_fc = self.layers['sub_fc7'] \n","        sbox = self.sbox\n","        sub_sp = self.sub_sp\n","        #_________________________Obj_feature\n","        ob_cls_prob = self.predictions['ob_cls_prob']\n","        ob_cls_pred = self.predictions['ob_cls_pred']\n","        ob_fc = self.layers['ob_fc7']\n","        obox = self.obox\n","        ob_sp = self.ob_sp\n","        #_____________________________Spatial\n","        co_occur = self.co_occurency_log\n","        union_box = self.union_box\n","        iou = self.iou\n","        sub_sp_info = tf.concat([iou, sbox, sub_sp, union_box], axis=1)\n","        ob_sp_info  = tf.concat([iou, obox, ob_sp , union_box], axis=1)\n","        #________________________________Word2Vec\n","        vector_dic = tf.Variable(self.object_word, trainable=False, name='VD_vo')\n","        sub_onehot = tf.matmul(tf.one_hot(self.sub_label, self.num_classes - 1), vector_dic)\n","        obj_onehot = tf.matmul(tf.one_hot(self.obj_label, self.num_classes - 1), vector_dic)\n","#______________________________________________________4. Balanced Size of extracted features\n","        sub_sp_info  = slim.fully_connected(sub_sp_info,  1000, activation_fn=tf.nn.relu, scope='RD_subinfo_fc',     weights_regularizer=self.max_norm_regularizer(threshold=0.85))\n","        sub_cls_prob = slim.fully_connected(sub_cls_prob, 500, activation_fn=tf.nn.relu, scope='RD_subclsprob_fc',  weights_regularizer=self.max_norm_regularizer(threshold=0.85))\n","        sub_fc       = slim.fully_connected(sub_fc,      3000, activation_fn=tf.nn.relu, scope='RD_subfc_fc',       weights_regularizer=self.max_norm_regularizer(threshold=0.85))\n","        label_s      = slim.fully_connected(sub_onehot , 1000, activation_fn=tf.nn.relu, scope='RD_ls1',            weights_regularizer=self.max_norm_regularizer(threshold=0.85))\n","\n","        ob_sp_info   = slim.fully_connected(ob_sp_info,   1000, activation_fn=tf.nn.relu, scope='RD_obinfo_fc',      weights_regularizer=self.max_norm_regularizer(threshold=0.85))\n","        ob_cls_prob  = slim.fully_connected(ob_cls_prob,  500, activation_fn=tf.nn.relu, scope='RD_obclsprob_fc',   weights_regularizer=self.max_norm_regularizer(threshold=0.85))\n","        ob_fc        = slim.fully_connected(ob_fc,       3000, activation_fn=tf.nn.relu, scope='RD_obfc_fc',        weights_regularizer=self.max_norm_regularizer(threshold=0.85))\n","        label_o      = slim.fully_connected(obj_onehot , 1000, activation_fn=tf.nn.relu, scope='RD_lo1',            weights_regularizer=self.max_norm_regularizer(threshold=0.85))       \n","#______________________Drop out\n","        sub_sp_info  = tf.layers.dropout(sub_sp_info , rate=0.2, training=True)\n","        sub_cls_prob = tf.layers.dropout(sub_cls_prob, rate=0.2, training=True)\n","        sub_fc       = tf.layers.dropout(sub_fc\t\t , rate=0.2, training=True)\n","        label_s \t = tf.layers.dropout(label_s\t , rate=0.2, training=True)\n","\n","        ob_sp_info  = tf.layers.dropout(ob_sp_info\t, rate=0.2, training=True)\n","        ob_cls_prob = tf.layers.dropout(ob_cls_prob\t, rate=0.2, training=True)\n","        ob_fc \t\t= tf.layers.dropout(ob_fc\t\t, rate=0.2, training=True)\n","        label_s \t= tf.layers.dropout(label_s\t\t, rate=0.2, training=True)\n","\n","        if self.index_sp:\n","            sub_fc = tf.concat([sub_fc,label_s, sub_sp_info, sub_cls_prob], axis=1)\n","            ob_fc  = tf.concat([ob_fc ,label_o, ob_sp_info , ob_cls_prob ], axis=1)\n","        if self.index_cls:\n","            sub_fc = tf.concat([sub_fc,label_s, sub_sp_info, sub_cls_prob], axis=1)\n","            ob_fc  = tf.concat([ob_fc ,label_o , ob_sp_info, ob_cls_prob ], axis=1)\n","        #________________________________________Embedding space\n","        sub_fc = slim.fully_connected(sub_fc, 2000, activation_fn=tf.nn.relu, scope='RD_sub_fc',weights_regularizer=self.max_norm_regularizer(threshold=0.85))\n","        ob_fc  = slim.fully_connected(ob_fc , 2000, activation_fn=tf.nn.relu, scope='RD_ob_fc' ,weights_regularizer=self.max_norm_regularizer(threshold=0.85))\n","        sub_fc = tf.layers.dropout(sub_fc, rate=0.2, training=True)\n","        ob_fc  = tf.layers.dropout(ob_fc , rate=0.2, training=True)\n","#__________________________________________________________________Sampling Head and tail       \n","        s = tf.shape(self.head)\n","        batch_idx = tf.tile(tf.expand_dims(tf.range(s[0]), 1), [1, s[1]])\n","        sub_head = tf.gather_nd(params=sub_fc, indices=tf.stack([batch_idx, self.head], axis=-1), name='RD_sub_head')\n","        sub_tail = tf.gather_nd(params=sub_fc, indices=tf.stack([batch_idx, self.tail], axis=-1), name='RD_sub_tail')\n","        ob_head  = tf.gather_nd(params=ob_fc,  indices=tf.stack([batch_idx, self.head], axis=-1), name='RD_ob_head')\n","        ob_tail  = tf.gather_nd(params=ob_fc,  indices=tf.stack([batch_idx, self.tail], axis=-1), name='RD_ob_tail')\n","\n","        sub_head = slim.fully_connected(sub_head, 2000, activation_fn=tf.nn.relu, scope='RD_subhead_fc',weights_regularizer=self.max_norm_regularizer(threshold=0.85))\n","        ob_head  = slim.fully_connected(ob_head , 2000, activation_fn=tf.nn.relu, scope='RD_obhead_fc' ,weights_regularizer=self.max_norm_regularizer(threshold=0.85))\n","        sub_tail = slim.fully_connected(sub_tail, 2000, activation_fn=tf.nn.relu, scope='RD_subtail_fc',weights_regularizer=self.max_norm_regularizer(threshold=0.85))\n","        ob_tail  = slim.fully_connected(ob_tail , 2000, activation_fn=tf.nn.relu, scope='RD_obtail_fc' ,weights_regularizer=self.max_norm_regularizer(threshold=0.85))\n","        \n","        sub_head = tf.layers.dropout(sub_head, rate=0.2, training=True)\n","        ob_head  = tf.layers.dropout(ob_head , rate=0.2, training=True)  \n","        sub_tail = tf.layers.dropout(sub_tail, rate=0.2, training=True)\n","        ob_tail  = tf.layers.dropout(ob_tail , rate=0.2, training=True)      \n","        #___________________________________1.VSimplE \n","        vsimple_vec = ((ob_tail - sub_head) + (sub_tail - ob_head)) / 2.0\n","#_________________________________________________________________________      Internal Language\n","        # sub_obj_p = slim.fully_connected(self.sub_obj_p, 500, activation_fn=tf.nn.relu, scope='RD_l1', reuse=False)\n","        # vsimple_vec = tf.concat([vsimple_vec, sub_obj_p], axis=1)\n","        vsimple_vec = slim.fully_connected(vsimple_vec, 2000, activation_fn=tf.nn.relu ,scope='RD_vsimple',weights_regularizer=self.max_norm_regularizer(threshold=0.9))\n","        vsimple_vec = tf.layers.dropout(vsimple_vec, rate=0.2, training=True)        \n","#___________________________________________________________________6.Bi_GRU\n","        gru_vec = tf.concat([sub_fc, vsimple_vec, ob_fc], axis=1)\n","        gru_vec = slim.fully_connected(gru_vec, 2000, activation_fn=tf.nn.relu, scope='RD_fc_GRU',weights_regularizer=self.max_norm_regularizer(threshold=0.9))\n","        gru_vec = tf.layers.dropout(gru_vec, rate=0.2, training=True)\n","        gru_vec = tf.expand_dims(gru_vec, 0)\n","        gru_vec = self.Bi_GRU(gru_vec, \"RD_GRU\")\n","\n","        PMI_score   = slim.fully_connected(co_occur   , 500, activation_fn=tf.nn.relu, scope='RD_fc1',weights_regularizer=self.max_norm_regularizer(threshold=0.9))\n","        vsimple_vec = slim.fully_connected(vsimple_vec, 500, activation_fn=tf.nn.relu, scope='RD_fc2',weights_regularizer=self.max_norm_regularizer(threshold=0.9))\n","        gru_vec     = slim.fully_connected(gru_vec    , 500, activation_fn=tf.nn.relu, scope='RD_fc3',weights_regularizer=self.max_norm_regularizer(threshold=0.9))\n","        #______________________Drop out\n","        vsimple_vec = tf.layers.dropout(vsimple_vec , rate=0.2, training=True)\n","        PMI_score   = tf.layers.dropout(PMI_score   , rate=0.2, training=True)\n","        gru_vec     = tf.layers.dropout(gru_vec     , rate=0.2, training=True)\n"," # _________________________________________________________________13.Score of VsimplE & GRU\n","        vsimple_score = slim.fully_connected(vsimple_vec, self.num_predicates, activation_fn=None, scope='RD_vsimple_score')\n","        gru_score     = slim.fully_connected(gru_vec    , self.num_predicates, activation_fn=None, scope='RD_gru_score')\n","        self.layers['vsimple_score'] = vsimple_score\n","        self.layers['gru_score']     = gru_score   \n","#___________________________________________________________________7. Feature Fusion\n","        rela_vec    = tf.concat([PMI_score, vsimple_vec, gru_vec], axis=1)\n","        rela_vec    = slim.fully_connected(rela_vec, 500, activation_fn=tf.nn.relu, scope='RD_fc4',weights_regularizer=self.max_norm_regularizer(threshold=1.0))\n","        rela_score  = slim.fully_connected(rela_vec, self.num_predicates, activation_fn=None, scope='RD_fc5')     \n","        self.layers['rela_score'] = rela_score\n","################################________________MULTI Class CALSSIFICATION___________________#############################################\n","        rela_prob = tf.nn.softmax(rela_score)      \n","        self.layers['rela_prob'] = rela_prob\n","##############################__________________MULTI LABEL CALSSIFICATION___________________#############################################\n","#         self.predictions['rela_pred_all'] = tf.concat([tf.nn.sigmoid(rela_score)], axis = 1)\n","#         rela_label \t= tf.one_hot( self.rela_label, self.num_predicates)\n","#         self.layers['rd_loss'] = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits( labels=rela_label, logits=rela_score))\n","\n","    def add_rd_loss(self):\n","################################________________MULTI Class CALSSIFICATION___________________#############################################\n","        gru_score   = self.layers['gru_score']\n","        rela_score  = self.layers['rela_score']\n","        rela_label  = self.rela_label\n","        vsimple_score = self.layers['vsimple_score']\n","        #_____________________________________________1.vsimple_Loss\n","        vsimple_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=rela_label, logits=vsimple_score))\n","        # ____________________________________________2.gru_Loss\n","        gru_loss     = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=rela_label, logits=gru_score))\n","        #_____________________________________________3.Total_Loss\n","        rd_loss      = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=rela_label, logits=rela_score))\n","#_____________________________________________15.L2_regularizeation__________________________________________________________________________________________      \n","        # regularizer = tf.contrib.layers.l2_regularizer(0.0001)\n","        # we = tf.trainable_variables()\n","        # RD_var = [var for var in we if 'RD' in var.name]\n","        # rd_loss = tf.contrib.layers.apply_regularization(regularizer, weights_list=RD_var) + rd_loss\n","#_______________________________________________________________________________________________________________________________________      \n","        acc_each = tf.nn.in_top_k(rela_score, rela_label, 1)\n","        self.losses['gru_loss']= gru_loss\n","        self.losses['rd_loss'] = rd_loss\n","        self.losses['acc_each']= acc_each\n","        self.losses['acc']     = tf.reduce_mean(tf.cast(acc_each, tf.float32))\n","        self.losses['vsimple_loss']  = vsimple_loss\n","\n","##############################_______________MULTI LABEL CALSSIFICATION___________________#############################################\n","#         gru_score = self.layers['gru_score']\n","#         vsimple_score = self.layers['vsimple_score']\n","#         rela_score = tf.nn.sigmoid(self.layers['rela_score'])\n","#         # regularizer = tf.contrib.layers.l2_regularizer(0.0001)\n","#         # we = tf.trainable_variables()\n","#         # RD_var = [var for var in we if 'RD' in var.name]\n","#         rela_label = tf.one_hot(self.rela_label, self.num_predicates)\n","#         # rd_loss = tf.contrib.layers.apply_regularization(regularizer, weights_list=RD_var) + self.layers['rd_loss']\n","#         rd_loss = self.layers['rd_loss']\n","#         prediction = tf.one_hot(tf.argmax(rela_score, 1), self.num_predicates)\n","#         correct_prediction = tf.not_equal(tf.argmax(tf.multiply(prediction, tf.cast(rela_label, tf.float32)), 1), False)\n","#         rela_pred = tf.argmax(rela_score, 1)\n","#         rela_max_prob = tf.reduce_max(rela_score, 1)\n","#         # _____________________________________________2.vsimple_Loss\n","#         #vsimple_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=rela_label, logits=vsimple_score))\n","#         vsimple_loss = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits( labels=rela_label, logits=vsimple_score))\n","#         # ____________________________________________3.gru_Loss\n","#         #gru_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=rela_label, logits=gru_score))\n","#         gru_loss = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits( labels=rela_label, logits=gru_score))\n","#         # _____________________________________________4.Total_Loss\n","#         #acc_each = tf.nn.in_top_k(rela_score, rela_label, 1)\n","#         self.losses['gru_loss'] = gru_loss\n","#         self.losses['vsimple_loss'] = vsimple_loss\n","#         self.losses['rd_loss'] = rd_loss\n","#         #self.losses['acc_each'] = acc_each\n","#         self.losses['rd_loss'] = rd_loss\n","#         self.predictions['rela_pred'] = rela_pred\n","#         self.predictions['rela_max_prob'] = rela_max_prob\n","#         self.losses['acc'] = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n","\n","    def train_predicate(self, sess, roidb_use, RD_train):\n","        im, im_scale = im_preprocess(roidb_use['image'])\n","        batch_num = len(roidb_use['index_pred'])/self.N_each_batch;print(\"##########@@@@@@@@@@@@###########\tbatch_num\t: \",batch_num)\n","        gru_loss = 0.0\n","        vsimple_loss = 0.0        \n","        RD_loss = 0.0\n","        acc = 0.0\n","        for batch_id in range(np.int32(batch_num)):\n","            blob = get_blob_pred(roidb_use, im_scale, self.index_sp, self.N_each_batch, batch_id)\n","            feed_dict = {self.image: im, self.co_occurency_log : blob['co_occurency_tensor'],\n","\t\t\t\t\t\t self.sbox: blob['sub_box'], self.obox: blob['obj_box'] ,\n","\t\t\t\t\t\t self.iou : blob['iou'] , self.union_box : blob['union_box'],\n","\t\t\t\t\t\t self.sub_sp : blob['sub_spatial_vector'] ,self.ob_sp:blob['ob_spatial_vector'],\n","\t\t\t\t\t\t self.rela_label: blob['rela'],self.keep_prob: 0.5,\n","                         self.sub_label: blob['sub_label'],self.obj_label: blob['obj_label'],\n","                         self.sub_obj_p: self.sub_obj_pred[blob['sub_label'], blob['obj_label']]}\n","\n","            _, losses = sess.run([RD_train, self.losses], feed_dict = feed_dict)\n","            \n","            gru_loss = gru_loss + losses['gru_loss']\n","            vsimple_loss = vsimple_loss + losses['vsimple_loss']     \n","            RD_loss = RD_loss + losses['rd_loss']\n","            acc = acc + losses['acc']\n","        gru_loss = gru_loss/batch_num\n","        vsimple_loss = vsimple_loss/batch_num\n","        RD_loss = RD_loss/batch_num\n","        acc = acc/batch_num\n","        return gru_loss, vsimple_loss, RD_loss, acc\n","\n","    def val_predicate(self, sess, roidb_use):\n","        im, im_scale = im_preprocess(roidb_use['image'])\n","        batch_num = len(roidb_use['index_pred'])/self.N_each_batch\n","        gru_loss = 0.0\n","        vsimple_loss = 0.0\n","        RD_loss = 0.0\n","        acc = 0.0\n","        for batch_id in range(np.int32(batch_num)):\n","            blob = get_blob_pred(roidb_use, im_scale, self.index_sp, self.N_each_batch, batch_id)\n","            feed_dict = {self.image: im, self.co_occurency_log : blob['co_occurency_tensor'],\n","\t\t\t\t\t\t self.sbox: blob['sub_box'], self.obox: blob['obj_box'],\n","\t\t\t\t\t\t self.iou: blob['iou'], self.union_box: blob['union_box'],\n","\t\t\t\t\t\t self.sub_sp : blob['sub_spatial_vector'] ,self.ob_sp:blob['ob_spatial_vector'],\n","\t\t\t\t\t\t self.rela_label: blob['rela'],self.keep_prob: 1,\n","                         self.sub_label: blob['sub_label'],self.obj_label: blob['obj_label'],\n","                         self.sub_obj_p: self.sub_obj_pred[blob['sub_label'], blob['obj_label']]}\n","\n","            losses = sess.run(self.losses, feed_dict=feed_dict)\n","\n","            gru_loss = gru_loss + losses['gru_loss']\n","            vsimple_loss = vsimple_loss + losses['vsimple_loss']\n","            RD_loss = RD_loss + losses['rd_loss']\n","            acc = acc + losses['acc']\n","        gru_loss = gru_loss/batch_num\n","        vsimple_loss = vsimple_loss/batch_num\n","        RD_loss = RD_loss/batch_num\n","        acc = acc/batch_num\n","        return  gru_loss, vsimple_loss, RD_loss, acc\n","\n","def max_pool(x, h, w, s_y, s_x, name, padding='SAME'):\n","    return tf.nn.max_pool(x, ksize=[1, h, w, 1], strides=[1, s_x, s_y, 1], padding=padding, name=name)\n","# _____________________________________________________________________________________________________#\n","N_cls = cfg.VRD_NUM_CLASS\n","N_rela = cfg.VRD_NUM_RELA\n","N_each_batch = cfg.VRD_BATCH_NUM\n","lr_init = cfg.VRD_LR_INIT\n","index_sp = False\n","index_cls = False\n","tf.reset_default_graph()\n","vnet = VSimplE()\n","vnet.create_graph(N_each_batch, index_sp, index_cls, N_cls, N_rela)\n","roidb_path = cfg.DIR + 'input/vrd_roidb.npz'\n","print(roidb_path)\n","res_path = cfg.DIR + 'Faster_RCNN/vrd_vgg_pretrained.ckpt'\n","print(res_path)\n","roidb_read = read_roidb(roidb_path)\n","train_roidb = roidb_read['train_roidb']\n","test_roidb = roidb_read['test_roidb']\n","N_train = len(train_roidb)\n","N_test = len(test_roidb)\n","N_round = 10\n","N_show = 100\n","N_save = N_train\n","N_val = N_test\n","total_var = tf.trainable_variables()\n","restore_var = [var for var in total_var if 'vgg_16' in var.name]\n","cls_score_var = [var for var in total_var if 'cls_score' in var.name]\n","res_var = [item for item in restore_var if item not in cls_score_var]\n","saver_res = tf.train.Saver(var_list=restore_var)\n","RD_var = [var for var in total_var if 'RD' in var.name]\n","saver = tf.train.Saver(max_to_keep=200)\n","for var in RD_var:\n","    print(var)\n","#___________________________________________________________________16.Adaptive Learning rate\n","# lr_init = 0.0001\n","# global_step = tf.Variable(0, trainable = False)\n","# learning_rate = tf.train.exponential_decay(lr_init, global_step = global_step, decay_steps = 4000, decay_rate = 0.5)\n","# learning_rate = tf.reduce_max([learning_rate, 0.000001])\n","#___________________________________________________________________Optimazer\n","optimizer = tf.train.AdamOptimizer(learning_rate=lr_init)\n","train_loss = vnet.losses['rd_loss'] #+ vnet.losses['vsimple_loss'] + vnet.losses['gru_loss']\n","RD_train = optimizer.minimize(train_loss, var_list = RD_var)   #, global_step = global_step\n","\n","os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n","config = tf.ConfigProto()\n","clip_all_weights = tf.get_collection(\"max_norm\")\n","\n","with tf.Session(config=config) as sess:\n","    init = tf.global_variables_initializer()\n","    sess.run(init)\n","    sess.run(clip_all_weights)\n","    saver_res.restore(sess, res_path)\n","    Epoch = 0\n","    gru_loss = 0.0\n","    vsimple_loss = 0.0\n","    rd_loss = 0.0\n","    acc = 0.0\n","    for Epoch in range(2):\n","        t=0    \n","        for r in range(N_round):\n","            for roidb_id in range(N_train):\n","                roidb_use = train_roidb[roidb_id]\n","                if len(roidb_use['rela_gt']) == 0:\n","                    continue\n","                gru_loss_temp, vsimple_loss_temp, rd_loss_temp, acc_temp = vnet.train_predicate(sess,roidb_use,RD_train)\n","                gru_loss = gru_loss + gru_loss_temp\n","                vsimple_loss = vsimple_loss + vsimple_loss_temp\n","                rd_loss = rd_loss + rd_loss_temp\n","                acc = acc + acc_temp\n","                t = t + 1\n","                if t % N_show == 0:\n","                    #lr = sess.run(learning_rate)\n","                    print(\"Epoch:{0}__ T:{1}>>> GRU : {2}| VSimplE : {3} | RD: {4} ___ Acc : {5} \".format(Epoch+1,t,round(gru_loss/N_show,2),round(vsimple_loss/N_show,2), round(rd_loss/N_show, 2),round(acc, 2)))\n","                    gru_loss = 0.0\n","                    vsimple_loss = 0.0\n","                    rd_loss = 0.0\n","                    acc = 0.0\n","\n","                if t % N_save == 0:\t\t\t\t\n","                    save_path = cfg.DIR + 'Model_ckpt/Pred' + format(int(t / N_save), '04') + '.ckpt'\n","                    saver.save(sess, save_path)\n","                    print(\"Model Saved to {0}\".format(save_path))\n","                    gru_loss_val = 0.0\n","                    vsimple_loss_val = 0.0\n","                    rd_loss_val = 0.0\n","                    acc_val = 0.0\n","                    for val_id in range(N_val):\n","                        roidb_use = test_roidb[val_id]\n","                        if len(roidb_use['rela_gt']) == 0:\n","                            continue\n","                        gru_loss_temp, vsimple_loss_temp, rd_loss_temp, acc_temp = vnet.val_predicate(sess,roidb_use)\n","\n","                        gru_loss_val = gru_loss + gru_loss_temp\n","                        vsimple_loss_val = vsimple_loss + vsimple_loss_temp\n","                        rd_loss_val = rd_loss_val + rd_loss_temp\n","                        acc_val = acc_val + acc_temp\n","                    print(\"\\n************************************GRU_Loss : {0} , VSimplE_Loss : {1} ,Total_Loss: {2} ____ Acc: {3}\\n\".format(round(gru_loss_val / N_val, 2), round(vsimple_loss_val / N_val, 2), round(rd_loss_val / N_val, 2), round(100 * acc_val/ N_val, 4)))\n","print(\" \\nDone!    Distance to best result \", round((100 * acc_val / N_val) - 60), 2)"],"execution_count":0,"outputs":[]}]}